{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9094738,"sourceType":"datasetVersion","datasetId":5488440}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nimport random\nimport numpy as np\nimport torch.nn as nn\nimport math\nimport argparse\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertForSequenceClassification, AutoTokenizer\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nfrom tqdm import tqdm\nscores = []","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:07:46.592650Z","iopub.execute_input":"2024-08-08T07:07:46.593479Z","iopub.status.idle":"2024-08-08T07:07:52.247716Z","shell.execute_reply.started":"2024-08-08T07:07:46.593447Z","shell.execute_reply":"2024-08-08T07:07:52.246747Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class Create_Dataset(Dataset):\n    def __init__(self, data, tokenizer, max_token_len: int = 256):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_token_len = max_token_len\n        self.text = \"\"\n\n    def __len__(self):\n        return len(self.data)\n\n    def getText(self):\n        return self.text\n\n    def __getitem__(self, index):\n        item = self.data['encoded_label'].values\n        title = self.data[\"text\"].values\n        text = str(title[index])\n        self.text = text\n        label = torch.tensor(item[index])\n        encoding = self.tokenizer.encode_plus(text,\n                                              add_special_tokens=True,\n                                              return_tensors='pt',\n                                              truncation=True,\n                                              max_length=self.max_token_len,\n                                              padding='max_length',\n                                              return_attention_mask=True\n                                              )\n        return {'input_ids': encoding.input_ids.flatten(), 'attention_mask': encoding.attention_mask.flatten()}, label\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-04T06:25:00.293148Z","iopub.execute_input":"2024-08-04T06:25:00.293502Z","iopub.status.idle":"2024-08-04T06:25:00.302035Z","shell.execute_reply.started":"2024-08-04T06:25:00.293474Z","shell.execute_reply":"2024-08-04T06:25:00.300986Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"\nclass Training:\n    def __init__(self, model_name, data_path, epoch, batch, max_len, lr,\n                 weight_decay, warmup, seed, device,save_directory):\n        self.model_name = model_name\n        self.data_path = data_path\n        self.epoch = epoch\n        self.batch = batch\n        self.max_len = max_len\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.warmup = warmup\n        self.seed = seed\n        self.device = device\n        self.save_directory = save_directory\n        \n        self.split_size=0.10\n        self.labels=None\n\n        self.set_seed()\n        \n\n    def set_seed(self):\n        torch.manual_seed(self.seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        random.seed(self.seed)\n        np.random.seed(self.seed)\n\n    def data_preparation(self,data_path,batch,test_size):\n        df=pd.read_csv(data_path)\n        labels=df[\"label\"].unique()\n        self.labels=labels\n        self.label2id, self.id2label=create_label2id_id2label(labels)\n        df['encoded_label'] = df['label'].apply(lambda x: self.label2id[x])\n        print(\"Class create success\")\n        print(df.encoded_label.value_counts())\n        if any(df.encoded_label.value_counts() <= 2):\n            traindf, valdf = train_test_split(df, test_size=test_size, random_state=42)\n        else:\n            traindf, valdf = train_test_split(df, test_size=test_size, random_state=42,\n                                              stratify=df.encoded_label)\n        \n        train_dataset = Create_Dataset(traindf, self.tokenizer)\n        val_dataset = Create_Dataset(valdf, self.tokenizer)\n    \n        train_dataloader = DataLoader(train_dataset, batch_size=batch, num_workers=4, shuffle=True)\n        val_dataloader = DataLoader(val_dataset, batch_size=batch, num_workers=4, shuffle=False)\n        print(f\"train:{traindf.shape} val:{valdf.shape} \")\n\n        return train_dataloader,val_dataloader\n    \n\n    def root_training(self):\n        self.tokenizer=AutoTokenizer.from_pretrained(self.model_name)\n        train_dataloader,val_dataloader=self.data_preparation(self.data_path,self.batch,self.split_size)\n        n_labels = len(self.labels)\n        model = BertForSequenceClassification.from_pretrained(self.model_name, num_labels=n_labels, id2label=self.id2label,\n                                                              label2id=self.label2id)\n        device = self.device\n        num_epochs = self.epoch\n        weight_decay = self.weight_decay\n        warmup = self.warmup\n        lr = self.lr\n        best_val_loss = float('inf')\n        patience = 2\n        model.to(device)\n        total_steps = len(train_dataloader) * num_epochs\n        warmup_steps = math.floor(total_steps * warmup)\n        warmup_steps = max(1, warmup_steps)\n\n        print(device)\n        # Model ve kayıp fonksiyonu\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=warmup_steps, T_mult=1)\n\n        running_loss = 0.0\n        model_name = \"model\"\n        for epoch in range(num_epochs):\n            model.train()\n            running_loss = 0.0\n            num_batches = len(train_dataloader)\n            with tqdm(total=num_batches, desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\") as pbar:\n                for batch_idx, batch in enumerate(train_dataloader):\n                    inputs, labels = batch\n                    input_ids = inputs[\"input_ids\"].to(device)\n                    attention_mask = inputs[\"attention_mask\"].to(device)\n                    labels = labels.to(device)\n                    optimizer.zero_grad()\n                    outputs = model(input_ids, attention_mask, labels=labels)\n                    loss = criterion(outputs.logits, labels)\n                    loss.backward()\n                    optimizer.step()\n                    scheduler.step(epoch + batch_idx / num_batches)\n                    running_loss += loss.item()\n                    # Ayrıntılı bilgi gösterme\n                    pbar.set_postfix({\"loss\": loss.item(), \"running_loss\": running_loss / (batch_idx + 1)})\n                    pbar.update(1)  # İlerleme çubuğunu güncelleme\n            # Doğrulama\n            model.eval()\n            val_loss = 0.0\n            for batch_idx, batch in enumerate(val_dataloader):\n                inputs, labels = batch\n                input_ids = inputs[\"input_ids\"].to(device)\n                attention_mask = inputs[\"attention_mask\"].to(device)\n                labels = labels.to(device)\n                with torch.no_grad():\n                    outputs = model(input_ids, attention_mask, labels=labels)\n                    loss = criterion(outputs.logits, labels)\n                val_loss += loss.item()\n                # Early Stopping kontrolü\n            val_loss /= len(val_dataloader)\n            name = \"Root_Model.pth\"\n            model_dir = \"models\"\n            if not os.path.exists(model_dir):\n                os.makedirs(model_dir)\n            model_path = os.path.join(model_dir, \"best_model.pth\")\n            # En iyi modeli kaydetme\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                model_save_path = os.path.join(self.save_directory, \"best_model\")\n                model.save_pretrained(model_save_path)\n                self.tokenizer.save_pretrained(model_save_path)\n            print(f\"Epoch [{epoch + 1}/{self.epoch}], Train Loss: {running_loss / num_batches:.4f}, Validation Loss: {val_loss:.4f}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T11:53:54.985777Z","iopub.execute_input":"2024-08-03T11:53:54.986167Z","iopub.status.idle":"2024-08-03T11:53:55.013976Z","shell.execute_reply.started":"2024-08-03T11:53:54.986137Z","shell.execute_reply":"2024-08-03T11:53:55.013003Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def data_preparation(data_path,batch,test_size,tokenizer):\n    df=pd.read_csv(data_path)\n    labels=df[\"label\"].unique()\n    label2id, id2label=create_label2id_id2label(labels)\n    df['encoded_label'] = df['label'].apply(lambda x: label2id[x])\n    print(\"Class create success\")\n    print(df.encoded_label.value_counts())\n    \n    if any(df.encoded_label.value_counts() <= 2):\n        traindf, valdf = train_test_split(df, test_size=test_size, random_state=42)\n\n    else:\n        traindf, valdf = train_test_split(df, test_size=test_size, random_state=42,\n                                          stratify=df.encoded_label)\n    tokenizer=AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n    train_dataset = Create_Dataset(traindf, tokenizer)\n    val_dataset = Create_Dataset(valdf, tokenizer)\n\n    train_dataloader = DataLoader(train_dataset, batch_size=batch, num_workers=4, shuffle=True)\n    val_dataloader = DataLoader(val_dataset, batch_size=batch, num_workers=4, shuffle=False)\n    print(f\"train:{traindf.shape} val:{valdf.shape} \")\n    \n    return train_dataloader,val_dataloader\n\n\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-08-04T06:24:39.708081Z","iopub.execute_input":"2024-08-04T06:24:39.708464Z","iopub.status.idle":"2024-08-04T06:24:39.717212Z","shell.execute_reply.started":"2024-08-04T06:24:39.708434Z","shell.execute_reply":"2024-08-04T06:24:39.716279Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def create_label2id_id2label(labels):\n    # Etiketleri alfabetik olarak sırala\n    sorted_labels = sorted(labels)\n    \n    # label2id ve id2label sözlüklerini oluştur\n    label2id = {label: idx for idx, label in enumerate(sorted_labels)}\n    id2label = {idx: label for idx, label in enumerate(sorted_labels)}\n    \n    return label2id, id2label","metadata":{"execution":{"iopub.status.busy":"2024-08-04T06:07:54.225446Z","iopub.execute_input":"2024-08-04T06:07:54.226287Z","iopub.status.idle":"2024-08-04T06:07:54.231513Z","shell.execute_reply.started":"2024-08-04T06:07:54.226253Z","shell.execute_reply":"2024-08-04T06:07:54.230514Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model_name=\"dbmdz/bert-base-turkish-cased\"\ndata_path=\"/kaggle/input/product-sentiment/df_all.csv\"\nepoch=5\nbatch=32\nmax_len=512\nlr=3e-5\nweight_decay=3e-4\nwarmup=0.2\nseed=42\ndevice=\"cuda\"\nsave_directory=\"/kaggle/working/\"\ntest_size=0.10\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T06:07:56.170634Z","iopub.execute_input":"2024-08-04T06:07:56.171000Z","iopub.status.idle":"2024-08-04T06:07:56.176272Z","shell.execute_reply.started":"2024-08-04T06:07:56.170972Z","shell.execute_reply":"2024-08-04T06:07:56.175390Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"training = Training(\n    model_name,\n    data_path,\n    epoch,\n    batch,\n    max_len,\n    lr,\n    weight_decay,\n    warmup,\n    seed,\n    device,\n    save_directory\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training.root_training()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T11:53:55.064316Z","iopub.execute_input":"2024-08-03T11:53:55.064595Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51b5d53a3fde4a2abebfe8b1d3e5d4fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9dbeeb381314dab83e03661f7ffbacb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/251k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f0bd67aeb2547bb88e0e792b867b184"}},"metadata":{}},{"name":"stdout","text":"Class create success\nencoded_label\n2    235949\n1    153825\n0     50905\nName: count, dtype: int64\ntrain:(396611, 5) val:(44068, 5) \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92292269436b4dcd9a9a2b4b758bbbea"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"cuda\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 12395/12395 [2:37:14<00:00,  1.31batch/s, loss=0.00547, running_loss=0.0962]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/5], Train Loss: 0.0962, Validation Loss: 0.0801\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 12395/12395 [2:37:23<00:00,  1.31batch/s, loss=0.0171, running_loss=0.0652]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/5], Train Loss: 0.0652, Validation Loss: 0.0753\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5:  41%|████      | 5101/12395 [1:04:45<1:32:56,  1.31batch/s, loss=0.0536, running_loss=0.0448] ","output_type":"stream"}]},{"cell_type":"code","source":"train_dataloader,val_dataloader=data_preparation(data_path,batch,test_size,tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T06:08:14.379568Z","iopub.execute_input":"2024-08-04T06:08:14.379938Z","iopub.status.idle":"2024-08-04T06:08:18.637069Z","shell.execute_reply.started":"2024-08-04T06:08:14.379909Z","shell.execute_reply":"2024-08-04T06:08:18.635786Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Class create success\nencoded_label\n2    235949\n1    153825\n0     50905\nName: count, dtype: int64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f9e91a4ddb34b549032d22a1d010ac2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47243c8abd12414f9ddc7f7d3984213a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/251k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c082057e92384268b538856bbb1073d3"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataloader,val_dataloader\u001b[38;5;241m=\u001b[39m\u001b[43mdata_preparation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[6], line 15\u001b[0m, in \u001b[0;36mdata_preparation\u001b[0;34m(data_path, batch, test_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m     traindf, valdf \u001b[38;5;241m=\u001b[39m train_test_split(df, test_size\u001b[38;5;241m=\u001b[39mtest_size, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m     13\u001b[0m                                       stratify\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mencoded_label)\n\u001b[1;32m     14\u001b[0m tokenizer\u001b[38;5;241m=\u001b[39mAutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbmdz/bert-base-turkish-cased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCreate_Dataset\u001b[49m(traindf, tokenizer)\n\u001b[1;32m     16\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m Create_Dataset(valdf, tokenizer)\n\u001b[1;32m     18\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mNameError\u001b[0m: name 'Create_Dataset' is not defined"],"ename":"NameError","evalue":"name 'Create_Dataset' is not defined","output_type":"error"}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification,pipeline","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:07:58.682540Z","iopub.execute_input":"2024-08-08T07:07:58.683082Z","iopub.status.idle":"2024-08-08T07:08:11.199819Z","shell.execute_reply.started":"2024-08-08T07:07:58.683050Z","shell.execute_reply":"2024-08-08T07:08:11.199053Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-08-08 07:08:01.264264: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-08 07:08:01.264407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-08 07:08:01.402927: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n\n# Hugging Face token ile giriş yapın\nlogin(\"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\")\n\n\n# Model ve tokenizer yolunu belirtin\nmodel_path = \"/kaggle/working/best_model\"\n\n# Modeli ve tokenizer'ı yükleyin\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# Model hub'a push'lamak için repository ismi belirleyin\nrepo_name = \"moarslan/bert-base-turkish-sentiment-analysis\"\n\n# Modeli ve tokenizer'ı push'layın\nmodel.push_to_hub(repo_name)\ntokenizer.push_to_hub(repo_name)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T07:09:20.658204Z","iopub.execute_input":"2024-08-08T07:09:20.658866Z","iopub.status.idle":"2024-08-08T07:09:39.921097Z","shell.execute_reply.started":"2024-08-08T07:09:20.658829Z","shell.execute_reply":"2024-08-08T07:09:39.920191Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0568a156a754e568505137cadad6d1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a9f9a2f14bb4b3184301b784306cb41"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/moarslan/bert-base-turkish-sentiment-analysis/commit/e71fba7bf502c319c04bc044e4484ea275579cf6', commit_message='Upload tokenizer', commit_description='', oid='e71fba7bf502c319c04bc044e4484ea275579cf6', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\"/kaggle/working/best_model\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/best_model\")","metadata":{"execution":{"iopub.status.busy":"2024-08-04T06:09:06.125129Z","iopub.execute_input":"2024-08-04T06:09:06.125935Z","iopub.status.idle":"2024-08-04T06:09:06.311331Z","shell.execute_reply.started":"2024-08-04T06:09:06.125904Z","shell.execute_reply":"2024-08-04T06:09:06.310555Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/product-sentiment/df_all.csv\")\nlabels=sorted(df[\"label\"].unique())\nprint(labels)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T06:18:16.297122Z","iopub.execute_input":"2024-08-04T06:18:16.297967Z","iopub.status.idle":"2024-08-04T06:18:18.723484Z","shell.execute_reply.started":"2024-08-04T06:18:16.297934Z","shell.execute_reply":"2024-08-04T06:18:18.722196Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"['Negative', 'Notr', 'Positive']\n","output_type":"stream"}]},{"cell_type":"code","source":"    from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n    def eval_root():\n        loaded_model = AutoModelForSequenceClassification.from_pretrained(\"/kaggle/working/best_model\")\n        tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/best_model\")\n        df=pd.read_csv(\"/kaggle/input/product-sentiment/df_all.csv\")\n        labels=sorted(df[\"label\"].unique())\n        train_dataloader,val_dataloader=data_preparation(\"/kaggle/input/product-sentiment/df_all.csv\",32,0.1,tokenizer)\n        print(labels)\n        device = \"cuda\"\n\n\n        n_labels = len(labels)\n        test_dataloader = val_dataloader\n        \n        true_labels = []\n        predicted_labels = []\n        loaded_model.to(device)\n        loaded_model.eval()\n        with tqdm(total=len(test_dataloader), unit=\"batch\") as pbar:\n            for batch in test_dataloader:\n                inputs, labels = batch\n                input_ids = inputs[\"input_ids\"].to(device)\n                attention_mask = inputs[\"attention_mask\"].to(device)\n                labels = labels.to(device)\n                with torch.no_grad():\n                    outputs = loaded_model(input_ids, attention_mask, labels=labels)\n                    logits = outputs.logits\n                    batch_predictions = torch.argmax(logits, axis=1)\n                    true_labels.extend(labels.tolist())\n                    predicted_labels.extend(batch_predictions.tolist())\n                pbar.update(1)\n        f1 = f1_score(true_labels, predicted_labels, average='macro')\n        print(\"F1 Skoru:\", f1)\n\n        recall = recall_score(true_labels, predicted_labels, average='macro')\n        print(\"Recall Skoru:\", recall)\n\n        precision = precision_score(true_labels, predicted_labels, average='macro')\n        print(\"Precision Skoru:\", precision)\n\n        accuracy = accuracy_score(true_labels, predicted_labels)\n        print(\"Accuracy:\", accuracy)\n        score = {\"f1\": f1, \"recall\": recall, \"precision\": precision, \"accuracy\": accuracy}\n        return score\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T06:31:50.064498Z","iopub.execute_input":"2024-08-04T06:31:50.065159Z","iopub.status.idle":"2024-08-04T06:31:50.077071Z","shell.execute_reply.started":"2024-08-04T06:31:50.065128Z","shell.execute_reply":"2024-08-04T06:31:50.076119Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"sentiment_analysis_pipeline = pipeline(\"sentiment-analysis\", model=models, tokenizer=tokenizers)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T05:55:46.819459Z","iopub.status.idle":"2024-08-04T05:55:46.819796Z","shell.execute_reply.started":"2024-08-04T05:55:46.819634Z","shell.execute_reply":"2024-08-04T05:55:46.819649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text= \"Bu kitapçık temel bilgiler içeriyor.\"","metadata":{"execution":{"iopub.status.busy":"2024-08-03T21:47:43.140572Z","iopub.execute_input":"2024-08-03T21:47:43.141396Z","iopub.status.idle":"2024-08-03T21:47:43.145018Z","shell.execute_reply.started":"2024-08-03T21:47:43.141365Z","shell.execute_reply":"2024-08-03T21:47:43.144069Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"sentiment_analysis_pipeline(text)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T21:47:43.291388Z","iopub.execute_input":"2024-08-03T21:47:43.291856Z","iopub.status.idle":"2024-08-03T21:47:43.356329Z","shell.execute_reply.started":"2024-08-03T21:47:43.291825Z","shell.execute_reply":"2024-08-03T21:47:43.355505Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"[{'label': 'Notr', 'score': 0.997557520866394}]"},"metadata":{}}]},{"cell_type":"code","source":"eval_root()","metadata":{"execution":{"iopub.status.busy":"2024-08-04T06:31:53.021703Z","iopub.execute_input":"2024-08-04T06:31:53.022534Z","iopub.status.idle":"2024-08-04T06:37:30.557545Z","shell.execute_reply.started":"2024-08-04T06:31:53.022503Z","shell.execute_reply":"2024-08-04T06:37:30.556498Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Class create success\nencoded_label\n2    235949\n1    153825\n0     50905\nName: count, dtype: int64\ntrain:(396611, 5) val:(44068, 5) \n['Negative', 'Notr', 'Positive']\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/1378 [00:00<?, ?batch/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n100%|█████████▉| 1377/1378 [05:30<00:00,  4.16batch/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n100%|██████████| 1378/1378 [05:31<00:00,  4.16batch/s]\n","output_type":"stream"},{"name":"stdout","text":"F1 Skoru: 0.9548370318776405\nRecall Skoru: 0.9521375413838135\nPrecision Skoru: 0.9576279141052209\nAccuracy: 0.9739947354089135\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"{'f1': 0.9548370318776405,\n 'recall': 0.9521375413838135,\n 'precision': 0.9576279141052209,\n 'accuracy': 0.9739947354089135}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}